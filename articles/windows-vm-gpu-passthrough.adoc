[float]
= Windows VM with GPU Passthrough

It is becoming more and more viable to create a Windows VM in Linux that utilizes a GPU.
The motivation to do this is for many programmers who are also gamers.
To have Linux as a development environment, and Windows for gaming.
If you're looking for how I did it, you can skip down to the <<execution, execution>> section.

When I first started programming, I was on Windows, but soon discovered the elegance of Linux for development.
This first taste was on a Linux virtual machine with a Windows host, quick, easy, non-distruptive.
But I became tired of the lag in the Linux virtual machine as well as the lack of real administration I could do to my machine via Linux.
So I moved onto dual booting Linux and Windows on my machine.
Which was great, but with the caveat that whenever I wanted to game, I needed to restart my computer.

This story is common for programmers who are also gamers.
I would like to choronicle another chapter for the benefit of others.
Setting up a Linux host with a Windows VM that has a GPU passed in for gaming.

== Hardware Setup and Prerequisites

To do this, the following is necessary:

* a CPU that supports virtualization and Directed I/O
* A motherboard that supports virtualization
* 2 video cards

For video cards, I would recommend ones of different brands (for example one AMD one NVIDIA) as it simplifies blacklisting drivers.
Also a CPU with integrated graphics would count as a "video card", such as Intel CPUs (except the Xeons).
This article does not cover the case of identical/similar video cards.

Below is a table of the specific hardware I had for my PC.

.PC Hardware
|===
| CPU | AMD - Ryzen 5 1600 3.2GHz 6-Core Processor
| Motherboard | ASUS - STRIX B350-F GAMING ATX AM4 Motherboard
| Windows GPU | Gigabyte - GeForce GTX 1080 8GB WINDFORCE OC 8G Video Card
| Linux GPU | AMD - Radeon R6450 1GB PCI-E
| RAM | G.Skill - Trident Z RGB 16GB (2 x 8GB) DDR4-3200 Memory
| SSD | Samsung - 850 EVO-Series 250GB 2.5" Solid State Drive
| HDD | BarraCuda - 1TB HDD, SATA III w/ 64MB Cache
| Power Supply | EVGA - SuperNOVA G2 750W 80+ Gold Certified Fully-Modular ATX Power Supply
|===

From this setup, I also had 2 monitors.
The monitor I wanted to game on was connected to the NVIDIA GTX 1080 card, and the other monitor was on the Radeon R6450.

As a non-hardware prerequisite, I set up a mouse and keyboard to interact with the VM via https://symless.com/synergy/downloads[_Synergy_], this does have a small monetary cost but is well worth it.

[[execution]]
== Execution

=== Enable Virtualization

The first step was to enable virtualization in the BIOS.
The setting to look for varies depending on the BIOS version, motherboard, and CPU.
In my case virtualization was enabled by default.

I checked by looking for AMD-Vi entries in dmesg.

[source, bash]
----
~ $ dmesg | grep AMD-Vi
[    1.073664] AMD-Vi: IOMMU performance counters supported
[    1.075695] AMD-Vi: Found IOMMU at 0000:00:00.2 cap 0x40
[    1.075695] AMD-Vi:  Extended features:  PPR NX GT IA GA PC
[    1.075698] AMD-Vi: Interrupt remapping enabled
[    1.075822] AMD-Vi: Lazy IO/TLB flushing enabled
----

=== Enable Modules

Next was adding the following to `/etc/modules`.

[source, txt]
----
pci_stub
vfio
vfio_iommu_type1
vfio_pci
vhost-net
kvm
kvm_amd
----

The last line will be `kvm_intel` for intel CPUs.
`pci-stub` allows the capture of the GPU and passing it to the VM.

=== Determine GPU PCI IDs

Next was getting the PCI IDs for both the and video card and corresponding audio component.
I did this with `lspci -nn | grep -A1 VGA`.
Searching for `VGA` found the video cards, and `-A1` showed the line below as well, which likely shows the corresponding audio component.

This was my output:

[source, bash]
----
~ $ lspci -nn | grep -A1 VGA
26:00.0 VGA compatible controller [0300]: Advanced Micro Devices, Inc. [AMD/ATI] Caicos [Radeon HD 6450/7450/8450 / R5 230 OEM] [1002:6779]
26:00.1 Audio device [0403]: Advanced Micro Devices, Inc. [AMD/ATI] Caicos HDMI Audio [Radeon HD 6400 Series] [1002:aa98]
27:00.0 VGA compatible controller [0300]: NVIDIA Corporation Device [10de:1b80] (rev a1)
27:00.1 Audio device [0403]: NVIDIA Corporation Device [10de:10f0] (rev a1)
----

So I can see my AMD card and my NVIDIA card.
I wanted to stub out the NVIDIA card because that's the one I planned to use for the VM.
The important values there were `[10de:1b80]` and `[10de:10f0]`, for video and audio respectively.

=== Kernel Boot Changes


With this new information, the kernel must be loaded up differently.
This can all be done by editing the GRUB command line arguments.
I did this by editing the `/etc/default/grub` file and adding arguments to the `GRUB_CMDLINE_LINUX` variable.
There are 3 things that must be done.

. The video card being passed through must be blacklisted.
To blacklist the video card, I used `nouveau.blacklist=1`.
`nouveau` is for nvidia cards, `radeon` for AMD.
. IOMMU groups must be enabled.
To enable IOMMU support in the kernel at boot time add `amd_iommu=on`, or `intel_iommu=on` for Intel CPUs.
. The video card being passed through must be stubbed out.
To stub out the video card with `pci-stub`, add `pci-stub.ids=10de:1b80,10de:10f0` (using the IDs from the previous step).

This is what my `/etc/default/grub/` file ended up looking like.
[source, txt]
----
GRUB_DEFAULT=0
GRUB_HIDDEN_TIMEOUT=0
GRUB_HIDDEN_TIMEOUT_QUIET=true
GRUB_TIMEOUT=10
GRUB_DISTRIBUTOR=`lsb_release -i -s 2> /dev/null || echo Debian`
GRUB_CMDLINE_LINUX_DEFAULT="quiet splash"
GRUB_CMDLINE_LINUX="nouveau.modeset=0 nouveau.blacklist=1 amd_iommu=on pci-stub.ids=10de:1b80,10de:10f0"
----

NOTE: I needed _nouveau.modeset=0_ before setting up all this otherwise loading into Linux would crash.

Then `sudo update-grub` will load the changes into grub with the new variables.

=== Reboot and Check

After all these changes, a reboot was necessary to load up the new kernel changes.
With all these changes, calling `lsmod | grep vfio`, `dmesg | grep pci-stub`, `dmesg | grep VFIO` should give output similar to the following.

[source, bash]
----
~ $ lsmod | grep vfio
vfio_pci               40960  0
vfio_virqfd            16384  1 vfio_pci
irqbypass              16384  2 kvm,vfio_pci
vfio_iommu_type1       20480  0
vfio                   28672  2 vfio_iommu_type1,vfio_pci
~ $ dmesg | grep pci-stub
[    0.000000] Command line: BOOT_IMAGE=/boot/vmlinuz-4.8.0-53-generic root=UUID=fc95e2a4-e179-4e36-85fb-eedcbbc50dfb ro nouveau.modeset=0 nouveau.blacklist=1 amd_iommu=on pci-stub.ids=10de:1b80,10de:10f0 quiet splash vt.handoff=7
[    0.000000] Kernel command line: BOOT_IMAGE=/boot/vmlinuz-4.8.0-53-generic root=UUID=fc95e2a4-e179-4e36-85fb-eedcbbc50dfb ro nouveau.modeset=0 nouveau.blacklist=1 amd_iommu=on pci-stub.ids=10de:1b80,10de:10f0 quiet splash vt.handoff=7
[    4.094259] pci-stub: add 10DE:1B80 sub=FFFFFFFF:FFFFFFFF cls=00000000/00000000
[    4.094268] pci-stub 0000:27:00.0: claimed by stub
[    4.094274] pci-stub: add 10DE:10F0 sub=FFFFFFFF:FFFFFFFF cls=00000000/00000000
[    4.094281] pci-stub 0000:27:00.1: claimed by stub
~ $ dmesg | grep VFIO
[    4.096253] VFIO - User Level meta-driver version: 0.3
----

=== Install VM Packages

I installed the following packages for my VM.

[source, bash]
----
sudo apt-get install qemu-kvm qemu-utils qemu-efi ovmf libvirt-bin libvirt-dev libvirt0 virt-manager gir1.2-spice-client-gtk-3.0
----

QEMU is the actual virtual machine software.
OVMF is a port of Intel's tianocore firmware to the QEMU virtual machine.
Spice is a display the QEMU will use before the display via the GPU is setup.

=== Get an ISO

For my Windows VM, I used the Windows 10 Anniversary ISO.
At the time of writing, this could be downloaded for free from the https://www.microsoft.com/en-ca/software-download/windows10ISO[Microsoft website].

=== Creating the VM

I used the GUI `virt-manager` to create the VM.
This was installed in the previous step.

After installing windows, in _Boot Options_, I changed the boot device order to boot from _IDE Disk 1_ so that it would boot off the installed Windows on the virtual hard drive.

From there I installed and set up https://symless.com/synergy/downloads[_Synergy_].
I also needed to install the http://www.nvidia.com/Download/index.aspx?lang=en-us[NVIDIA driver] before I could see anything on the display connected to the VM card.
Once _Synergy_ and the driver were setup, I turned off the VM and made the following hardware changes.

Remove

* Display Spice
* Video Cirrus

Add

* The VM graphics card through the PCI Host Device option, I added both the video card and audio component.

In addition to making the hardware changes, the VM needed some configuration in the XML as well.
This is because the NVIDIA driver for some cards (such as the 1080) fails if it detects a VM environment.
So to trick the driver into thinking it's not a VM, it just takes adding a parameter to the XML.
Through `virsh edit <vm-name>` I added the following `<kvm>` tag in to the `<features>` section.

[source,xml]
----
<features>
  ...
  <kvm>
    <hidden state='on' />
  </kvm>
  ...
</features>
----

Now when you turn on the VM, the gaming monitor should light up for the first time via the other graphics card.

== Future Work

So after all that, I successfully ran a Windows VM in Linux that used a physical GPU.
But there is more work to be done as it was more of an experimental motion for myself since I have decided not to use it as my everyday setup.
The outstanding issues are joint audio, disk drive passthrough, benchmarking, and a KVM AMD bug.

For joint audio, there are a number of proposed solutions online.
The most canonical solution, in the spirit of virtualization, would be to pass a virtual sound card from the host into the virtual machine.
Then pass the virtual sound card's output to an audio handler in the host (PulseAudio or ALSA).
The virtual sound card solution seemed to have varying levels of success for others online.
For myself, I want to take an easier solution by making use of the second audio-in slot on my speakers.
I would buy a physical USB sound card to plug into my system and pass it to the Windows VM, USB passthrough is well supported so this shouldn't be difficult.
Then I would connect the virtual sound card's output to my speaker's second audio in.

Another idea I would like to explore later is installing Windows on a second physical drive, and booting off of it in both VM and baremetal fashions.
Hard disk passthrough looks to be consistently done from other people online.
It is hard to say whether a single Windows installation would deal well with booting up in both a VM and baremetal environment.
I think the biggest concern is that the hardware visible to the Windows installation would change often.

There are some reports online of people getting higher benchmarks with GPU passthrough than on baremetal.
There is lots of customization methods that can be done for better virtualization such as RAM allocation with Hugepages and CPU pinning.
I would like to explore these myself and compare benchmarks in a virtualized environment verses a baremetal environment.

Lastly, the KVM AMD bug in short is a bug in the Linux KVM module where nested page tables seriously hinders GPU performance, but without nested page tables CPU performance is seriously hindered.
Before this is resolved, it is very difficult to get near baremetal performance in a Windows VM with an AMD Ryzen CPU.
But because it seems as though AMD wants to excel in the virtualization aspect, I expect this to be resolved in 2017 or early 2018.
You can read more about the link:https://forum.level1techs.com/t/ryzen-gpu-passthrough/116458[KVM AMD issue here].

== References

Very little of what I did here was truly original, I followed many different guides online.
Please take the time to look at these fantastic guides as well.

. https://ycnrg.org/vga-passthrough-with-ovmf-vfio/
. https://davidyat.es/2016/09/08/gpu-passthrough
. https://wiki.archlinux.org/index.php/PCI_passthrough_via_OVMF
