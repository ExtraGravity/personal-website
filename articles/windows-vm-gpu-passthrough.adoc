[float]
= Windows VM with GPU Passthrough

It is becoming more and more viable to create a Windows VM in Linux that utilizes a GPU.
The motivation to do this is for many programmers who are also gamers.
To have Linux as a development environment, and Windows for gaming.
If you're looking for how I did it, you can skip down to the <<execution, execution>> section.

When I first started programming, I was on Windows, but soon discovered the elegance of Linux for development.
This first taste was on a Linux virtual machine with a Windows host, quick, easy, non-distruptive.
But I became tired of the lag in the Linux virtual machine as well as the lack of real administration I could do to my machine via Linux.
So I moved onto dual booting Linux and Windows on my machine.
Which was great, but with the caveat that whenever I wanted to game, I needed to restart my computer.

This story is common for programmers who are also gamers.
I would like to choronicle another chapter for the benefit for others.
Setting up a Linux host with a Windows VM that has a GPU passed through for gaming.

== Hardware Setup and Prerequisites

To do this, the following is necessary:

* a CPU that supports virtualization and Directed I/O
* A motherboard that supports virtualization
* 2 video cards

For video cards, I would recommend different branded ones (for example one AMD one NVIDIA) as it simplifies blacklisting drivers.
Also a CPU with integrated graphics would count as one, such as Intel CPUs (except the Xeons).
This article does not cover the case of identical/similar video cards.

Below is a table of the specific hardware I had for my PC.

.PC Hardware
|===
| CPU | AMD - Ryzen 5 1600 3.2GHz 6-Core Processor
| Motherboard | ASUS - STRIX B350-F GAMING ATX AM4 Motherboard
| Windows GPU | Gigabyte - GeForce GTX 1080 8GB WINDFORCE OC 8G Video Card
| Linux GPU | AMD - Radeon R6450 1GB PCI-E
| RAM | G.Skill - Trident Z RGB 16GB (2 x 8GB) DDR4-3200 Memory
| SSD | Samsung - 850 EVO-Series 250GB 2.5" Solid State Drive
| HDD | BarraCuda - 1TB HDD, SATA III w/ 64MB Cache
| Power Supply | EVGA - SuperNOVA G2 750W 80+ Gold Certified Fully-Modular ATX Power Supply
|===

From this setup, I also had 2 monitors.
The monitor I want to game on was connected to the NVIDIA GTX 1080 card, and the other monitor on the Radeon R6450.

As a non-hardware prerequisite, I set up a mouse and keyboard to interact with the VM via https://symless.com/synergy/downloads[_Synergy_], this does have a small monetary cost but is well worth it.

For audio, I opted for a more hardware oriented solution compared to a software solution, you can read the <<audio, audio section>> for more specifics.

.Audio Hardware
|===
| Speakers | Logitech Z523
| Audio Mixer | Penstar BS-318
| USB Audio Card | TROND AC2 Aluminum USB Audio Adapter
|===

[[execution]]
== Execution

=== Environment Preperation for VM

==== Enable Virtualization

The first step was to enable virtualization in the BIOS.
The setting to look for varies depending on the BIOS version, motherboard, and CPU.
In my case virtualization was enabled by default.

I checked by looking for AMD-Vi entries in dmesg.

[source, bash]
----
~ $ dmesg | grep AMD-Vi
[    1.073664] AMD-Vi: IOMMU performance counters supported
[    1.075695] AMD-Vi: Found IOMMU at 0000:00:00.2 cap 0x40
[    1.075695] AMD-Vi:  Extended features:  PPR NX GT IA GA PC
[    1.075698] AMD-Vi: Interrupt remapping enabled
[    1.075822] AMD-Vi: Lazy IO/TLB flushing enabled
----

==== Enable Modules

Add the following to `/etc/modules`.

----
pci_stub
vfio
vfio_iommu_type1
vfio_pci
vhost-net
kvm
kvm_amd
----

The last line will be `kvm_intel` for intel CPUs.
`pci-stub` allows the capture of the GPU and passing it to the VM.

==== Determine GPU PCI IDs

Next was getting the PCI IDs for both the and video card and audio component.
I did this with `lspci -nn | grep -A1 VGA`.
Searching for `VGA` found the video cards, and `-A1` showed the line below as well, which is likely the audio components.

This was my output:
[source, bash]
----
~ $ lspci -nn | grep -A1 VGA
26:00.0 VGA compatible controller [0300]: Advanced Micro Devices, Inc. [AMD/ATI] Caicos [Radeon HD 6450/7450/8450 / R5 230 OEM] [1002:6779]
26:00.1 Audio device [0403]: Advanced Micro Devices, Inc. [AMD/ATI] Caicos HDMI Audio [Radeon HD 6400 Series] [1002:aa98]
27:00.0 VGA compatible controller [0300]: NVIDIA Corporation Device [10de:1b80] (rev a1)
27:00.1 Audio device [0403]: NVIDIA Corporation Device [10de:10f0] (rev a1)
----

So I can saw my AMD card and my NVIDIA card.
I wanted to stub out the NVIDIA card because that's the one I plan to use for the VM.
The important values there were `[10de:1b80]` and `[10de:10f0]`, for video and audio respectively.

==== Kernel Boot Changes


With this new information, the kernel must be loaded up differently.
This can all be done by editing the GRUB command line arguments.
I did this by editing the `/etc/default/grub` file and adding arguments to the `GRUB_CMDLINE_LINUX` variable.
There are 3 things that must be done.

. The video card being passed through must be blacklisted.
To blacklist the other video card, I used `nouveau.blacklist=1`.
`nouveau` is for nvidia cards, `radeon` for AMD.
. IOMMU groups must be enabled.
To enable IOMMU support in the kernel at boot time add `amd_iommu=on` (`intel_iommu=on` for intel CPUs).
. The video card being passed through must be stubbed out.
To stub out the video card with `pci-stub`, add `pci-stub.ids=10de:1b80,10de:10f0` (with the correct IDs from the previous step).

This is what my `/etc/default/grub/` file ended up looking like.
----
GRUB_DEFAULT=0
GRUB_HIDDEN_TIMEOUT=0
GRUB_HIDDEN_TIMEOUT_QUIET=true
GRUB_TIMEOUT=10
GRUB_DISTRIBUTOR=`lsb_release -i -s 2> /dev/null || echo Debian`
GRUB_CMDLINE_LINUX_DEFAULT="quiet splash"
GRUB_CMDLINE_LINUX="nouveau.modeset=0 nouveau.blacklist=1 amd_iommu=on pci-stub.ids=10de:1b80,10de:10f0"
----

NOTE: I needed `nouveau.modeset=0` before setting up all this otherwise loading into Linux would crash.

Finally, run `sudo update-grub` to update grub with the new variables.

=== Reboot and Check

After all these changes, a reboot was necessary to load up the new kernel changes.
With all these changes, calling the following: `lsmod | grep vfio`, `dmesg | grep pci-stub`, `dmesg | grep VFIO`, should give output similar to the following.

[source, bash]
----
~ $ lsmod | grep vfio
vfio_pci               40960  0
vfio_virqfd            16384  1 vfio_pci
irqbypass              16384  2 kvm,vfio_pci
vfio_iommu_type1       20480  0
vfio                   28672  2 vfio_iommu_type1,vfio_pci
~ $ dmesg | grep pci-stub
[    0.000000] Command line: BOOT_IMAGE=/boot/vmlinuz-4.8.0-53-generic root=UUID=fc95e2a4-e179-4e36-85fb-eedcbbc50dfb ro nouveau.modeset=0 nouveau.blacklist=1 amd_iommu=on pci-stub.ids=10de:1b80,10de:10f0 quiet splash vt.handoff=7
[    0.000000] Kernel command line: BOOT_IMAGE=/boot/vmlinuz-4.8.0-53-generic root=UUID=fc95e2a4-e179-4e36-85fb-eedcbbc50dfb ro nouveau.modeset=0 nouveau.blacklist=1 amd_iommu=on pci-stub.ids=10de:1b80,10de:10f0 quiet splash vt.handoff=7
[    4.094259] pci-stub: add 10DE:1B80 sub=FFFFFFFF:FFFFFFFF cls=00000000/00000000
[    4.094268] pci-stub 0000:27:00.0: claimed by stub
[    4.094274] pci-stub: add 10DE:10F0 sub=FFFFFFFF:FFFFFFFF cls=00000000/00000000
[    4.094281] pci-stub 0000:27:00.1: claimed by stub
~ $ dmesg | grep VFIO
[    4.096253] VFIO - User Level meta-driver version: 0.3
----

=== Setting up the VM

I installed the following packages for my VM.

    sudo apt-get install qemu-kvm qemu-utils qemu-efi ovmf libvirt-bin libvirt-dev libvirt0 virt-manager gir1.2-spice-client-gtk-3.0

QEMU is the actual virtual machine software.
OVMF is a port of Intel's tianocore firmware to the QEMU virtual machine.
Spice is a display the QEMU will use before the display via the GPU is setup.

==== Get your ISO

For my Windows VM, I used the Windows 10 Anniversary ISO.
At the time of writing, this could be downloaded for free from the https://www.microsoft.com/en-ca/software-download/windows10ISO[Microsoft website].

==== Creating the VM

I used the GUI `virt-manager` to create the VM.
This was installed in the previous step.
Here is some screenshots of my process.

After installing windows, in _Boot Options_, I changed the boot device order to boot from _IDE Disk 1_ so that it would boot off the installed Windows on the virtual hard drive.

From there I installed and set up https://symless.com/synergy/downloads[_Synergy_].
I also needed to installthe http://www.nvidia.com/Download/index.aspx?lang=en-us[NVIDIA driver] before I see anything on the display connected to the VM card.
Once _Synergy_ and the driver were setup, I turned off the VM and made the following hardware changes.

Remove

* Display Spice
* Video Cirrus

Add

* The VM graphics card through the PCI Host Device option, I added both the video card and audio component.

In addition to making the hardware changes, the KVM needed some configuration in the XML as well.
This is because the NVIDIA driver for some NVIDIA cards (such as the 1080) fails if it detects a VM environment.
So to trick the driver into thinking it's not a VM, it just takes adding a parameter to the XML.
Through `virsh edit <vm-name>` I added the following `<kvm>` tag in to the `<features>` section.

[source,xml]
----
<features>
  ...
  <kvm>
    <hidden state='on' />
  </kvm>
  ...
</features>
----

Now when you turn on the VM, the gaming monitor should light up for the first time via the other graphics card.
Then came the magical moment of turning on the VM and seeing the gaming monitor come alive.

[[audio]]
==== Audio

To get audio from the guest VM to the host, I opted for a more hardware oriented solution compared to a software solution.
Some solutions I've saw before included

* Running audio through a Spice display.
* Passing a virtual sound card and rerouting it to ALSA or Pulseaudio on the host.
* Using the video card's sound card to power HDMI audio.

==== Disk Drive

== References

None of what I did here was truly original, I followed many different guides online.
Please take the time to look at these other fantastic guides as well.

. https://ycnrg.org/vga-passthrough-with-ovmf-vfio/
. https://davidyat.es/2016/09/08/gpu-passthrough
. https://wiki.archlinux.org/index.php/PCI_passthrough_via_OVMF
